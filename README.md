# Hyperflow Executors

For all of the installation setup follow: https://github.com/hyperflow-wms/hyperflow/wiki/TutorialAMQP
except use this fork of Hyperflow: https://github.com/MF57/hyperflow 

## Direct Executor Model

In this model, hyperflow directly calls serverless function via HTTP Request. In Direct Executor Model directory there is a code of the serverless function (npm project) for both AWS Lambda and Google Cloud Functions with scripts which allow for function upload.

There are also scripts which allows function deployment in given cloud provider.

One function execution corresponds to one activity from Hyperflow workflow. It is divided into 3 parts:
- download phase - all input files of workflow activity are downloaded to local directory of the function from cloud provider's storage
- execution phase - the activity's binary is being executed
- upload phase - output files generated by the binary are uploaded to the cloud provider's storage

Note: All of the binaries which will be executed in the workflow have to be either deployed with the function (they have to be in the same directory as function code while deploying it) or be in cloud provider's storage (but then the workflow activities have to have them in their inputs array)

Function will either return 200 with some metrics data of the invocation or 400 with an error

### AWS Lambda
- Amazon S3 is used as storage (bucket and file location have to be passed in the function invocation)
- create-function.sh script creates new function; update-function.sh updates existing one
- there is a 0.5 GB limit of /tmp storage of a single function execution (single activity cannot have inputs + outputs + executable size over 0.5 GB)

### Google Cloud Funcions
- Google Cloud Storage is used as storage (bucket and file location have to be passed in the function invocation)
- deploy.sh creates new or updates existing function

## Queue Executor

In thi model, hyperflow puts workflow activities to the Rabbit MQ queue and there has to be some entity which puts these activities from the queue, executes them and responds to the Hyperflow.

### Local

In Local directory there is executor_config.yml with basic config of the https://github.com/hyperflow-wms/hyperflow/wiki/TutorialAMQP executor and starts.sh script which runs it.

### Task Invoker

The task invoker subscribes to the queue (RabbitMQ required), triggers the serverless function and responds to the Hyperflow.

Run it by:
```
node index.js aws
```
for AWS Lambda integration

or

```
node index.js gcf
```

for Google Cloud Functions integration

## Hybrid executor

In this model, Hyperflow puts workflow activities to several RabbitMQ queues based on the field in the workflow activity from the DAG. One kind of tasks could be run by Task Invoker, another one could be run by VM hyperflow-amqp-executor. The DAG file has to be decorated - all activities have to have field 'deploymentType' which then will be used by hyperflow to determine which queue to use for current task.

### Dag Decorator
```
node index.js <path_to_folder_where_dag.json_file_is>
```
This naive simple decorated takes dag file and assigns to all tasks 'lambda' as deploymentType except for the ones which name is listed in index.js - their deployment type will be 'vm'. Output file is dag_decorated.json
